{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\n",
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "%env SM_FRAMEWORK=tf.keras\n",
    "\n",
    "import segmentation_models as sm\n",
    "#from keras_segmentation.predict import predict\n",
    "import cv2\n",
    "#from matplotlib import pyplot as plt\n",
    "#from matplotlib import cm\n",
    "#from skimage.transform import radon, rescale, rotate\n",
    "#from skimage.morphology import (square, rectangle, diamond, disk, cube, octahedron, ball, octagon, star,\n",
    "#                                binary_closing, closing, opening, skeletonize, erosion, dilation)\n",
    "#from skimage.transform import probabilistic_hough_line\n",
    "#from skimage.feature import canny\n",
    "#from skimage.measure import label, regionprops, regionprops_table\n",
    "from skimage.io import imread, imsave\n",
    "#import math\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sklearn.metrics import jaccard_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available\", len(physical_devices))\n",
    "\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is E:\\Joao\\jocival_linha_plantio_run\n"
     ]
    }
   ],
   "source": [
    "def make_dirs(output_dir):\n",
    "    try:\n",
    "        os.makedirs(output_dir)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    else:\n",
    "        print (\"\\nSuccessfully created the directory %s \" % output_dir)\n",
    "\n",
    "# detect the current working directory and print it\n",
    "path = os.getcwd()\n",
    "print (\"The current working directory is %s\" % path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_model(DECODER, BACKBONE, input_shape, encoder_weights, encoder_freeze, classes, activation):\n",
    "def load_model(DECODER, model_args):\n",
    "    #model = sm.Unet(\n",
    "    #model = sm.Linknet(\n",
    "    #model = sm.PSPNet(\n",
    "\n",
    "    #    backbone_name=BACKBONE, input_shape=(x, x, 3), encoder_weights='imagenet',\n",
    "    #    encoder_freeze=True, classes=1, activation='sigmoid'\n",
    "    #)\n",
    "\n",
    "    ## input_shape â€“ shape of input data/image (H, W, C)\n",
    "        # Unet and Linknet - H and W of input images should be divisible by factor 32.\n",
    "        # PSPNet - H and W should be divisible by 6 * downsample_factor and NOT None! - downsample_factor = 8\n",
    "\n",
    "    if DECODER == \"Unet\" :\n",
    "        print(\"\\n1 DECODER:\", DECODER)\n",
    "        model = sm.Unet(**model_args)\n",
    "\n",
    "    elif DECODER == \"Linknet\":\n",
    "        print(\"\\n2 DECODER:\", DECODER)\n",
    "        model = sm.Linknet(**model_args)\n",
    "\n",
    "    elif DECODER == \"PSPNet\" :\n",
    "        print(\"\\n3 DECODER:\", DECODER)\n",
    "        model = sm.PSPNet(**model_args)\n",
    "    else :\n",
    "        raise ValueError(\"Error DECODER:\", DECODER)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights (fold_to_load = 1):\n",
    "    weights_to_load = weights_path + dataset_weights + '/' + BACKBONE + '_' + \\\n",
    "                  DECODER + '_' + dataset_weights + '_' + str(fold_to_load) + \".hdf5\"\n",
    "    print(\"weights_to_load:\", weights_to_load)\n",
    "\n",
    "    #model = sm.Linknet(backbone_name=BACKBONE, encoder_weights='imagenet', \n",
    "    #        encoder_freeze=True, classes=1, activation='sigmoid',\n",
    "    #        weights = 'dados/vgg16_linknet/vgg16_linknet_9.hdf5')\n",
    "\n",
    "    # size - target_size?: 240 PSPNet, 256 Unet and Linknet, 288 All\n",
    "    if DECODER == \"Unet\" or DECODER == \"Linknet\":\n",
    "        size = 256 # 240 # 256 # 288 ## target_size\n",
    "    elif DECODER == \"PSPNet\":\n",
    "        size = 240\n",
    "    else:\n",
    "        raise ValueError(\"Error DECODER:\", DECODER)\n",
    "\n",
    "    print(\"\\ntarget_size:\", size, size)\n",
    "\n",
    "    data_load_model_args = dict(backbone_name = BACKBONE, input_shape = (size, size, 3),\n",
    "                            encoder_weights='imagenet', encoder_freeze = True,\n",
    "                            classes = 1, activation = 'sigmoid', weights = weights_to_load)\n",
    "\n",
    "    model = load_model(DECODER, data_load_model_args)\n",
    "    return model\n",
    "    #model = sm.Unet(\n",
    "    #model = sm.Linknet(\n",
    "    #model = sm.PSPNet(\n",
    "    #    backbone_name = BACKBONE, encoder_weights = \"imagenet\", encoder_freeze = True,\n",
    "    #    classes = 1, activation = \"sigmoid\", weights = weights_to_load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "#import numpy as np\n",
    "\n",
    "#load images\n",
    "#y_pred = cv2.imread('predictions/image_001.png')\n",
    "#y_true = cv2.imread('ground_truth/image_001.png') \n",
    "\n",
    "# Dice similarity function\n",
    "def dice(y_pred, y_true, k = 1):\n",
    "    intersection = np.sum(y_pred[y_true==k]) * 2.0\n",
    "    dice = intersection / (np.sum(y_pred) + np.sum(y_true))\n",
    "    return dice\n",
    "\n",
    "#dice_score = dice(y_pred, y_true, k = 255) #255 in my case, can be 1 \n",
    "#print (\"Dice Similarity: {}\".format(dice_score))\n",
    "\n",
    "#def dice_coef2(y_pred, y_true):\n",
    "#    y_true_f = y_true.flatten()\n",
    "#    y_pred_f = y_pred.flatten()\n",
    "#    union = np.sum(y_true_f) + np.sum(y_pred_f)\n",
    "#    if union==0: return 1\n",
    "#    intersection = np.sum(y_true_f * y_pred_f)\n",
    "#    return 2. * intersection / union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diretorio = \"D:\\\\Datasets\\\\linha_plantio\\\\cortadas_2152\\\\\"\n",
    "#saida = \"D:\\\\Datasets\\\\linha_plantio\\\\cortadas_2152\\\\predicao/\"\n",
    "\n",
    "#diretorio = \"E:\\Jocival_Mapas\"\n",
    "\n",
    "#save_predict = False\n",
    "save_predict = True\n",
    "\n",
    "def make_predict():\n",
    "    #nr_files = len(glob.glob(diretorio + 'images/*.tif'))\n",
    "    nr_files = len(glob.glob(diretorio + dataset_predict + \"/*.png\"))\n",
    "    print(\"nr_files:\", nr_files)\n",
    "\n",
    "    dados_IOU = np.zeros([nr_files, 1])\n",
    "    dados_dice = np.zeros([nr_files, 1])\n",
    "\n",
    "    result_name = BACKBONE + '_' + DECODER + \"_weights_\" + dataset_weights + \"_predict_\" + \\\n",
    "                  dataset_predict + '_' + str(fold_to_load)\n",
    "    print(\"\\nresult_name:\", result_name)\n",
    "\n",
    "    make_dirs(output_dir)\n",
    "\n",
    "#     do10_Only = False\n",
    "#     do10_Only = True\n",
    "\n",
    "    i = 1\n",
    "    qtd = 0\n",
    "    IOU_ACC = 0.0\n",
    "    dice_ACC = 0.0\n",
    "\n",
    "    #for name in glob.glob(diretorio + 'images/*.tif'):\n",
    "    for name in sorted(glob.glob(diretorio + dataset_predict + \"/*.png\")):\n",
    "    #for name in sorted([os.path.basename(x) for x in glob.glob(os.path.join((diretorio + dataset + \"/*.png\")))]):\n",
    "        #print(\"img_name:\", name)\n",
    "        img = imread(name) #38 484\n",
    "        img = img / 255\n",
    "\n",
    "        #print(\"img\", img)\n",
    "        #print(\"img.shape\", img.shape) # (256, 256, 3)\n",
    "\n",
    "        if DECODER == \"PSPNet\" :\n",
    "            # resize image to 240, 240\n",
    "            img = cv2.resize(img, (240, 240), interpolation = cv2.INTER_AREA)\n",
    "            #print(\"img.shape\", img.shape) # (240, 240, 3)\n",
    "\n",
    "        pr = model.predict(np.array([img]))[0]\n",
    "        pr = np.array(pr)\n",
    "        pr = pr[:,:, 0]\n",
    "        pr[pr >= 0.5] = 1\n",
    "        pr[pr < 1] = 0\n",
    "        pr = pr.astype('uint8')\n",
    "\n",
    "        #mask = cv2.imread(diretorio + \"labels/\" + os.path.basename(name), 0)\n",
    "        mask = cv2.imread(diretorio + dataset_predict + \"_mask/\" + os.path.basename(name), 0)\n",
    "\n",
    "        #print(\"mask\", mask)\n",
    "        #print(\"mask.shape\", mask.shape) # (256, 256)\n",
    "        if DECODER == \"PSPNet\" :\n",
    "            # resize image mask\n",
    "            mask = cv2.resize(mask, (240, 240), interpolation = cv2.INTER_AREA)\n",
    "            #print(\"mask.shape\", mask.shape) # (240, 240)\n",
    "\n",
    "        mask[mask > 0] = 1\n",
    "\n",
    "        ##IOU = jaccard_score(mask.flatten(), pr.flatten(), 'binary')\n",
    "        IOU = jaccard_score(mask.flatten(), pr.flatten(), average = \"binary\")\n",
    "        #print (\"IOU Similarity: {}\".format(IOU))\n",
    "\n",
    "        dice_score = dice(pr, mask, k = 1) #1 in my case, can be 255 \n",
    "        #print (\"Dice Similarity: {}\".format(dice_score))\n",
    "\n",
    "        #dice_score = dice_coef2(pr, mask)\n",
    "        #print (\"Dice Similarity: {}\".format(dice_score))\n",
    "\n",
    "        dados_IOU[qtd] = IOU\n",
    "        IOU_ACC += IOU\n",
    "\n",
    "        dados_dice[qtd] = dice_score\n",
    "        dice_ACC += dice_score\n",
    "\n",
    "        qtd += 1\n",
    "        pr[pr == 1] = 255\n",
    "\n",
    "        if save_predict :\n",
    "            #print(\"name.split('/')[-1]:\", name.split('/')[-1])\n",
    "\n",
    "            name = name.split('/')[-1]\n",
    "            name_predict = name.split('.')[0] + '_' + result_name + \".png\"\n",
    "            imsave(output_dir + os.path.basename(name_predict), pr, check_contrast = False)\n",
    "\n",
    "            #print(output_dir + os.path.basename(name_predict))\n",
    "\n",
    "            #if IOU <= 0.6: \n",
    "                #None\n",
    "\n",
    "        print(i, end=' ')\n",
    "        i += 1\n",
    "\n",
    "#         if do10_Only and i == 11 :\n",
    "#             break\n",
    "\n",
    "    #print(\"\\nIOU_ACC:\", IOU_ACC)\n",
    "    #print(\"IOU:\", IOU)\n",
    "    #print(\"qtd:\", qtd)\n",
    "    print(\"\\nIOU_ACC / qtd:\", IOU_ACC / qtd)\n",
    "\n",
    "    #print(\"\\ndice_ACC:\", dice_ACC)\n",
    "    #print(\"dice_score:\", dice_score)\n",
    "    #print(\"qtd:\", qtd)\n",
    "    print(\"\\ndice_ACC / qtd:\", dice_ACC / qtd)\n",
    "\n",
    "    np.savetxt(output_dir + result_name + \"_IOU.csv\", dados_IOU, delimiter = ',')\n",
    "    np.savetxt(output_dir + result_name + \"_dice.csv\", dados_dice, delimiter = ',')\n",
    "\n",
    "    return dados_IOU, dados_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv  \n",
    "\n",
    "def make_row (a, b, writer):\n",
    "    print(a, b)\n",
    "    if b == -10: # blank line\n",
    "        row = [] # blank line\n",
    "    elif b == -11:\n",
    "        row = [a]\n",
    "    else :\n",
    "        row = [a, b]\n",
    "\n",
    "    writer.writerow(row)\n",
    "    #print(\"row:\", row)\n",
    "    #return row\n",
    "\n",
    "def save_sumary(output_dir, dados_IOU, dados_dice):\n",
    "    # output_dir.rsplit('/',2)[0] - remove '/fold_?/' or /fold_??/', until second /\n",
    "    result_name_sumary = output_dir.rsplit('/', 2)[0] + '/' + BACKBONE + '_' + DECODER + \"_weights_\" + \\\n",
    "                         dataset_weights + \"_predict_\" + dataset_predict  + \"_sumary.csv\"\n",
    "\n",
    "    # open the file in the write mode\n",
    "    #f = open(output_dir + result_name + \"_sumary.csv\", 'w', encoding='UTF8')\n",
    "\n",
    "    if fold_to_load == 1:\n",
    "        mode_open = 'w'\n",
    "    else:\n",
    "        mode_open = 'a'\n",
    "\n",
    "    f = open(result_name_sumary, mode_open , newline='', encoding='UTF8')\n",
    "\n",
    "    # create the csv writer\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    #row = [\"np.average(dados_IOU):\", 0.00847634797536522]\n",
    "    #row = [\"np.average(dados_IOU):\", np.average(dados_IOU)]\n",
    "    #writer.writerow(row)\n",
    "\n",
    "    fold_run = output_dir.rsplit('/', 2)[1]\n",
    "\n",
    "    make_row('', -10, writer) # blank line\n",
    "\n",
    "    if fold_to_load == 1:\n",
    "        make_row(result_name_sumary, -11, writer)\n",
    "        make_row('', -10, writer)\n",
    "\n",
    "    make_row(fold_run, -11, writer)\n",
    "    make_row('', -10, writer)\n",
    "\n",
    "    make_row(\"np.average(dados_IOU):\", np.average(dados_IOU), writer)\n",
    "    #make_row(\"np.mean(dados_IOU):\", np.mean(dados_IOU), writer)\n",
    "    make_row(\"np.median(dados_IOU):\", np.median(dados_IOU), writer)\n",
    "    make_row(\"np.std(dados_IOU):\", np.std(dados_IOU), writer)\n",
    "    make_row(\"np.var(dados_IOU):\", np.var(dados_IOU), writer)\n",
    "    make_row('', -10, writer)\n",
    "\n",
    "    make_row(\"np.average(dados_dice):\", np.average(dados_dice), writer)\n",
    "    #make_row(\"np.mean(dados_dice):\", np.mean(dados_dice), writer)\n",
    "    make_row(\"np.median(dados_dice):\", np.median(dados_dice), writer)\n",
    "    make_row(\"np.std(dados_dice):\", np.std(dados_dice), writer)\n",
    "    make_row(\"np.var(dados_dice):\", np.var(dados_dice), writer)\n",
    "    make_row('', -10, writer)\n",
    "\n",
    "    # close the file\n",
    "    f.close()\n",
    "\n",
    "def load_values_from_csv():\n",
    "    # Load CSV and save sumary with average/median to dice and IOU\n",
    "\n",
    "    #import numpy as np\n",
    "    ##from numpy import genfromtxt\n",
    "\n",
    "    print(\"output_dir:\", output_dir)\n",
    "\n",
    "    result_name = BACKBONE + '_' + DECODER + \"_weights_\" + dataset_weights + \"_predict_\" + \\\n",
    "                  dataset_predict + '_' + str(fold_to_load)\n",
    "    print(\"result_name:\", result_name + \"_(IOU|dice).csv\\n\")\n",
    "\n",
    "    file_name = output_dir + result_name\n",
    "    #file_dice_name = output_dir + result_name + \"_dice.csv\"\n",
    "\n",
    "    dados_IOU = np.genfromtxt(file_name + \"_IOU.csv\", delimiter=',')\n",
    "    dados_dice = np.genfromtxt(file_name + \"_dice.csv\", delimiter=',')\n",
    "\n",
    "    return dados_IOU, dados_dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = \"vgg16\"\n",
    "\n",
    "# 4 * 4 * 10 * 3 = 480\n",
    "# 8 * 8 * 10 * 3 = 1920\n",
    "\n",
    "#dataset_weights = \"Base_A\"\n",
    "#dataset_weights = \"Base_B\"\n",
    "#dataset_weights = \"Base_C\"\n",
    "#dataset_weights = \"Base_D\"\n",
    "#dataset_weights = \"Base_E200\"\n",
    "#dataset_weights = \"Base_E300\"\n",
    "#dataset_weights = \"Base_E400\"\n",
    "dataset_weights = \"Base_E500\"\n",
    "\n",
    "#dataset_predict = \"Base_A\" ...\n",
    "#DECODER = \"Unet\" ...\n",
    "\n",
    "#weights_path = \"/media/sda2/home/j/Downloads/tests/results2/\"\n",
    "#weights_path = \"/media/sda2/home/j/Downloads/tests/correct/\"\n",
    "weights_path = \"E:/Joao/jocival_linha_plantio_run/results/\"\n",
    "\n",
    "#diretorio = \"/media/sda2/home/j/Downloads/tests/Recortes/\"\n",
    "diretorio = \"E:/Backes/Segmentacao Linha Plantio CNN/Recortes/\"\n",
    "\n",
    "folds = 10 #10\n",
    "\n",
    "model_make_predict = True # load_weights() and make_predict()\n",
    "#model_make_predict = False # load_values_from_csv()\n",
    "\n",
    "for k in range(8): #8\n",
    "    if k == 0:\n",
    "        dataset_predict = \"Base_A\"\n",
    "    elif k == 1:\n",
    "        dataset_predict = \"Base_B\"\n",
    "    elif k == 2:\n",
    "        dataset_predict = \"Base_C\"\n",
    "    elif k == 3:\n",
    "        dataset_predict = \"Base_D\"\n",
    "    elif k == 4:\n",
    "        dataset_predict = \"Base_E200\"\n",
    "    elif k == 5:\n",
    "        dataset_predict = \"Base_E300\"\n",
    "    elif k == 6:\n",
    "        dataset_predict = \"Base_E400\"\n",
    "    elif k == 7:\n",
    "        dataset_predict = \"Base_E500\"\n",
    "\n",
    "#for k in range(4):\n",
    "#    if k == 0:\n",
    "#        dataset_predict = \"Base_E200\"\n",
    "#    elif k == 1:\n",
    "#        dataset_predict = \"Base_E300\"\n",
    "#    elif k == 2:\n",
    "#        dataset_predict = \"Base_E400\"\n",
    "#    elif k == 3:\n",
    "#        dataset_predict = \"Base_E500\"\n",
    "\n",
    "    for j in range(3): #3\n",
    "        if j == 0:\n",
    "            DECODER = \"Unet\"\n",
    "        elif j == 1:\n",
    "            DECODER = \"Linknet\"\n",
    "        elif j == 2:\n",
    "            DECODER = \"PSPNet\"\n",
    "\n",
    "        for i in range(folds): #folds=10\n",
    "            print(\"\\n\\tDECODER\", DECODER, \"dataset_weights\", dataset_weights, \"dataset_predict\", dataset_predict)\n",
    "\n",
    "            fold_to_load = i + 1\n",
    "            print(\"\\n# fold:\", fold_to_load)\n",
    "            output_dir = \"results/predict/weights_\" + dataset_weights + \"/predict_\" + dataset_predict + '/' + \\\n",
    "                         DECODER + \"/fold_\" + str(fold_to_load) + '/'\n",
    "\n",
    "            if model_make_predict:\n",
    "                model = load_weights(fold_to_load)\n",
    "\n",
    "                dados_IOU, dados_dice = make_predict()\n",
    "            else:\n",
    "                dados_IOU, dados_dice = load_values_from_csv()\n",
    "\n",
    "            save_sumary(output_dir, dados_IOU, dados_dice)\n",
    "\n",
    "print(\"\\n\\nAll Done!\\n\")\n",
    "#E:\\Joao\\jocival_linha_plantio_run\\results\\predict\\weights_Base_A\\predict_Base_A\\Unet\\fold_1\n",
    " #   vgg16_Unet_Base_A_1_predict_dice.csv\n",
    "    \n",
    "#vgg16_Unet_weights_Base_D_predict_Base_D_1_IOU\n",
    "#vgg16_Unet_weights_Base_D_predict_Base_D_2_IOU\n",
    "#vgg16_Unet_Base_D_predict_1_IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#diretorio = \"/media/sda2/home/j/Downloads/tests/recortes_tmp/\"\n",
    "\n",
    "#DECODER = \"Unet\" ...\n",
    "\n",
    "def load_csv(DECODER):\n",
    "    log_to_load = weights_path + dataset + '/' + BACKBONE + '_' + DECODER + '_' + dataset + \".csv\"\n",
    "    print(\"log_to_load:\", log_to_load)\n",
    "\n",
    "    log_data = pd.read_csv(log_to_load, sep=',', engine='python')\n",
    "#     print(log_data)\n",
    "    return log_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_grah(log_data, grah_info, DECODER):\n",
    "    #print(log_data.keys())\n",
    "\n",
    "    info_data = log_data[grah_info]\n",
    "    val_info_data = log_data[\"val_\" + grah_info]\n",
    "\n",
    "    print(\"DECODER:\", DECODER)\n",
    "    print(\"graph:\", grah_info, \" - val_\", grah_info)\n",
    "\n",
    "    #loss_data = log_data[\"iou_score\"]\n",
    "    #val_loss_data = log_data[\"val_iou_score\"]\n",
    "\n",
    "    folds = 10\n",
    "    epoch = 50\n",
    "\n",
    "    info_mean_data = np.zeros(50)\n",
    "    val_info_mean_data = np.zeros(50)\n",
    "\n",
    "    line = 0\n",
    "    for i in range (folds * epoch):\n",
    "        info_mean_data[line] += info_data[i]\n",
    "        val_info_mean_data[line] += val_info_data[i]\n",
    "\n",
    "#         if line == 1:\n",
    "#             print(\"info_data[\",i,\"]:\", info_data[i])\n",
    "#             print(\"val_info_data[\",i,\"]:\", val_info_data[i])\n",
    "\n",
    "        line += 1\n",
    "        if line == 50:\n",
    "            line = 0\n",
    "\n",
    "#     print(info_mean_data)\n",
    "#     print(val_info_mean_data)\n",
    "#     print(info_data[50:100])\n",
    "\n",
    "    for i in range (epoch):\n",
    "        info_mean_data[i] /= 10\n",
    "        val_info_mean_data[i] /= 10\n",
    "\n",
    "    #print(info_mean_data)\n",
    "    #print(val_info_mean_data)\n",
    "\n",
    "    # summarize history for loss\n",
    "\n",
    "#     plt.plot(log_data['loss'][0:50])\n",
    "#     plt.plot(log_data['val_loss'][0:50])\n",
    "#     plt.title('model loss fold 1')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "\n",
    "    plt.plot(info_mean_data)\n",
    "    plt.plot(val_info_mean_data)\n",
    "    plt.title(DECODER +' model mean ' + grah_info)\n",
    "\n",
    "#     plt.ylabel('loss')\n",
    "    plt.ylabel(grah_info)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='best', frameon=False)\n",
    "\n",
    "    plt.grid(axis=\"y\")\n",
    "\n",
    "    #plt.ylim(ymin=0.5)\n",
    "    #plt.ylim(ymax=3)\n",
    "    \n",
    "    #plt.xlim(xmin=0)\n",
    "    #plt.xlim(xmax=50)\n",
    "\n",
    "    #output_path =  os.path.join(weights_path + dataset + '/')\n",
    "    output_path = weights_path + dataset + '/grah_info/' + BACKBONE + '_' + DECODER + '_' + dataset + '_' + grah_info\n",
    "\n",
    "    plt.savefig(output_path, dpi = 400)\n",
    "    print(\"saved to:\", output_path)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run make_graph for the 3 networks - docoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = \"Base_A\"\n",
    "#dataset = \"Base_B\"\n",
    "#dataset = \"Base_C\"\n",
    "#dataset = \"Base_D\"\n",
    "#dataset = \"Base_E200\"\n",
    "#dataset = \"Base_E300\"\n",
    "#dataset = \"Base_E400\"\n",
    "#dataset = \"Base_E500\"\n",
    "\n",
    "BACKBONE = \"vgg16\"\n",
    "\n",
    "weights_path = \"E:/Joao/jocival_linha_plantio_run/results/\"\n",
    "\n",
    "output_path = weights_path + dataset + '/grah_info/'\n",
    "make_dirs(output_path)\n",
    "\n",
    "for ii in range (3):\n",
    "    if ii == 0:\n",
    "        grah_info = \"loss\"\n",
    "    elif ii == 1:\n",
    "        grah_info = \"iou_score\"\n",
    "    else :\n",
    "        grah_info = \"f1-score\"\n",
    "\n",
    "    for jj in range(3):\n",
    "        if jj == 0:\n",
    "            DECODER_graph = \"Unet\"\n",
    "        elif jj == 1:\n",
    "            DECODER_graph = \"Linknet\"\n",
    "        else :\n",
    "            DECODER_graph = \"PSPNet\"\n",
    "\n",
    "        log_data_tmp = load_csv(DECODER_graph)\n",
    "\n",
    "        make_grah(log_data_tmp, grah_info, DECODER_graph)\n",
    "\n",
    "print(\"\\n\\nAll Done!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
